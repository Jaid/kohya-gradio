{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dF_YX7yRzAo",
        "outputId": "9eed8183-b1c3-44c4-d79e-dbe51aeb70d8"
      },
      "outputs": [],
      "source": [
        "# @title Check environment\n",
        "!env | sort\n",
        "!python --version\n",
        "!pip list\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9-0FBqw-WCE",
        "outputId": "fc624ba2-e576-4776-88ea-b6c59add4953"
      },
      "outputs": [],
      "source": [
        "# @title Clone\n",
        "!git clone https://github.com/Jaid/kohya-gradio.git\n",
        "%cd kohya-gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0gbHCLAhrk",
        "outputId": "816fdc6c-87e4-4a58-d0e4-e7a7302347df"
      },
      "outputs": [],
      "source": [
        "!python -m pip --disable-pip-version-check install --upgrade pip\n",
        "%pip install --disable-pip-version-check --requirement requirements_colab.txt --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.nvidia.com\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRybQMnBnzSY"
      },
      "outputs": [],
      "source": [
        "%mkdir --parents private\n",
        "import requests\n",
        "def download(url, target):\n",
        "  print(f\"{url} â†’ {target}\")\n",
        "  response = requests.get(url, allow_redirects=True)\n",
        "  response.raise_for_status()\n",
        "  with open(target, 'wb') as output_file:\n",
        "    output_file.write(response.content)\n",
        "download('https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors', 'private/vae.safetensors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uxo_u6AZydZ"
      },
      "outputs": [],
      "source": [
        "# @title Check CUDA support in PyTorch\n",
        "!pip show torch\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "!pip show tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uHk9V0EJqpi"
      },
      "outputs": [],
      "source": [
        "wandbKey = '' # @param {type:\"string\"}\n",
        "!accelerate launch sdxl_train_network.py --pretrained_model_name_or_path stabilityai/stable-diffusion-xl-base-1.0 --vae private/vae.safetensors --train_data_dir img --output_dir out/model --logging_dir out/log --resolution 1024,1024 --save_model_as safetensors --text_encoder_lr 0.0001 --unet_lr 0.0001 --network_dim 64 --network_alpha 32 --output_name trained --max_train_steps 80 --no_half_vae --caption_extension .txt --cache_latents --network_module lycoris.kohya --network_args preset=full algo=full rank_dropout=0 module_dropout=0 use_tucker=False use_scalar=False rank_dropout_scale=True train_norm=True --optimizer_type adamw --save_every_n_epochs 1 --lr_scheduler constant --xformers --mixed_precision bf16 --save_precision bf16 --metadata_title SinansWoche --log_with wandb --wandb_api_key {wandbKey}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

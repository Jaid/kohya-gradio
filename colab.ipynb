{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dF_YX7yRzAo",
        "outputId": "9eed8183-b1c3-44c4-d79e-dbe51aeb70d8"
      },
      "outputs": [],
      "source": [
        "# @title Check environment\n",
        "!env | sort\n",
        "!python --version\n",
        "!pip list\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9-0FBqw-WCE",
        "outputId": "fc624ba2-e576-4776-88ea-b6c59add4953"
      },
      "outputs": [],
      "source": [
        "# @title Clone\n",
        "import tempfile\n",
        "import os\n",
        "repoFolder = os.path.join(tempfile.gettempdir(), \"kohya-gradio\")\n",
        "!git clone https://github.com/Jaid/kohya-gradio.git \"$repoFolder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0gbHCLAhrk",
        "outputId": "816fdc6c-87e4-4a58-d0e4-e7a7302347df"
      },
      "outputs": [],
      "source": [
        "!python -m pip --disable-pip-version-check install --upgrade pip\n",
        "requirementsFile = os.path.join(repoFolder, \"requirements_colab.txt\")\n",
        "%pip install --requirement {requirementsFile} --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.nvidia.com\n",
        "%pip install shlex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uxo_u6AZydZ"
      },
      "outputs": [],
      "source": [
        "# @title Check CUDA support in PyTorch\n",
        "!pip show torch\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "!pip show tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRybQMnBnzSY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def download(url, target):\n",
        "  print(f\"{url} â†’ {target}\")\n",
        "  response = requests.get(url, allow_redirects=True)\n",
        "  response.raise_for_status()\n",
        "  with open(target, 'wb') as output_file:\n",
        "    output_file.write(response.content)\n",
        "vaeLocation = 'https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors' # @param {type:\"string\"}\n",
        "%mkdir inputModel\n",
        "download(vaeLocation, 'inputModel/vae.safetensors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!accelerate config default\n",
        "!accelerate test\n",
        "import accelerate\n",
        "print(accelerate.utils.get_max_memory())\n",
        "print(accelerate.utils.is_bf16_available())\n",
        "print(accelerate.utils.is_deepspeed_available())\n",
        "print(accelerate.utils.is_cuda_available())\n",
        "print(accelerate.utils.is_tpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uHk9V0EJqpi"
      },
      "outputs": [],
      "source": [
        "# @title Generate Command { run: \"auto\" }\n",
        "import math\n",
        "\n",
        "dim = 64 # @param {type:\"integer\"}\n",
        "imageCount = 4 # @param {type:\"integer\"}\n",
        "epochs = 3 # @param {type:\"integer\"}\n",
        "warmupStepsPercent = 0 # @param {type:\"integer\"}\n",
        "learningRate = 0.0000004 # @param {type:\"number\"}\n",
        "vramSaving = 'lossless' # @param [\"lossless\", \"lossy\", \"none\"]\n",
        "wandbKey = '' # @param {type:\"string\"}\n",
        "outputFormat = 'safetensors' # @param [\"safetensors\", \"ckpt\"]\n",
        "alwaysSave = True # @param {type:\"boolean\"}\n",
        "useLycoris = False # @param {type:\"boolean\"}\n",
        "adaptiveOptimizer = False # @param {type:\"boolean\"}\n",
        "samplePrompt = '' # @param {type:\"string\"}\n",
        "pruneOutput = False # @param {type:\"boolean\"}\n",
        "\n",
        "modelFolder = 'model'\n",
        "steps = imageCount * epochs\n",
        "alpha = math.trunc(dim / 2)\n",
        "\n",
        "if warmupStepsPercent:\n",
        "  warmupSteps = math.floor(steps * warmupStepsPercent / 100)\n",
        "else:\n",
        "  warmupSteps = 0\n",
        "\n",
        "if samplePrompt:\n",
        "  samplePrompts = samplePrompt.split('|')\n",
        "  sampleInstruction = samplePrompt + ' --w 1024 --h 1024 --l 6 --s 50 --d 1'\n",
        "  with open('prompt.txt', 'w') as file:\n",
        "    file.write(sampleInstruction)\n",
        "\n",
        "hasBf16 = accelerate.utils.is_bf16_available()\n",
        "hasCuda = accelerate.utils.is_cuda_available()\n",
        "\n",
        "accelerateArguments = [\n",
        "]\n",
        "\n",
        "launchArguments = [\n",
        "  '--pretrained_model_name_or_path',\n",
        "  # \"$rootMixed/private/checkpoint.safetensors\"\n",
        "  'stabilityai/stable-diffusion-xl-base-1.0',\n",
        "  '--vae',\n",
        "  \"inputModel/vae.safetensors\",\n",
        "  '--train_data_dir',\n",
        "  os.path.join(repoFolder, 'img'),\n",
        "  '--output_dir',\n",
        "  modelFolder,\n",
        "  '--logging_dir',\n",
        "  \"log\",\n",
        "  '--resolution',\n",
        "  '1024,1024',\n",
        "  '--save_model_as',\n",
        "  outputFormat,\n",
        "  '--text_encoder_lr',\n",
        "  '0.0001',\n",
        "  '--unet_lr',\n",
        "  '0.0001',\n",
        "  '--network_dim',\n",
        "  dim,\n",
        "  '--network_alpha',\n",
        "  alpha,\n",
        "  '--output_name',\n",
        "  'trained',\n",
        "  '--max_train_steps',\n",
        "  steps,\n",
        "  # '--max_train_epochs'\n",
        "  # 10\n",
        "  '--no_half_vae',\n",
        "  '--caption_extension',\n",
        "  '.txt',\n",
        "  '--cache_latents',\n",
        "]\n",
        "if useLycoris:\n",
        "  launchArguments += [\n",
        "    '--network_module',\n",
        "    'lycoris.kohya',\n",
        "    '--network_args',\n",
        "    'preset=full',\n",
        "    'algo=full',\n",
        "    'rank_dropout=0',\n",
        "    'module_dropout=0',\n",
        "    'use_tucker=False',\n",
        "    'use_scalar=False',\n",
        "    'rank_dropout_scale=True',\n",
        "    'train_norm=True',\n",
        "  ]\n",
        "else:\n",
        "  launchArguments += [\n",
        "    '--network_module',\n",
        "    'networks.lora',\n",
        "  ]\n",
        "if adaptiveOptimizer:\n",
        "  launchArguments += [\n",
        "    '--optimizer_type',\n",
        "    'adafactor',\n",
        "    '--optimizer',\n",
        "    'adafactor',\n",
        "    '--optimizer_args',\n",
        "    'scale_parameter=False',\n",
        "    'relative_step=False',\n",
        "    'warmup_init=False',\n",
        "  ]\n",
        "else:\n",
        "  launchArguments += [\n",
        "    '--optimizer_type',\n",
        "  ]\n",
        "  if vramSaving == 'lossy':\n",
        "    launchArguments += [\n",
        "      'adamw8bit',\n",
        "    ]\n",
        "  else:\n",
        "    launchArguments += [\n",
        "      'adamw',\n",
        "    ]\n",
        "if alwaysSave:\n",
        "  launchArguments += [\n",
        "    '--save_every_n_epochs',\n",
        "    1,\n",
        "  ]\n",
        "if not learningRate:\n",
        "  launchArguments += [\n",
        "    '--lr_scheduler',\n",
        "    'adafactor',\n",
        "  ]\n",
        "elif warmupSteps:\n",
        "  launchArguments += [\n",
        "    '--lr_scheduler',\n",
        "    'constant_with_warmup',\n",
        "    '--lr_warmup_steps',\n",
        "    warmupSteps,\n",
        "    '--learning_rate',\n",
        "    learningRate,\n",
        "  ]\n",
        "else:\n",
        "  launchArguments += [\n",
        "    '--lr_scheduler',\n",
        "    'constant',\n",
        "  ]\n",
        "if vramSaving != 'none':\n",
        "  launchArguments += [\n",
        "    '--lowram',\n",
        "  ]\n",
        "  if hasCuda:\n",
        "    launchArguments += [\n",
        "      '--xformers',\n",
        "    ]\n",
        "if vramSaving == 'lossy':\n",
        "  if hasBf16:\n",
        "    launchArguments += [\n",
        "      '--mixed_precision',\n",
        "      'bf16',\n",
        "      # '--full_bf16',\n",
        "    ]\n",
        "  else:\n",
        "    launchArguments += [\n",
        "      '--mixed_precision',\n",
        "      'fp16',\n",
        "      # '--full_fp16',\n",
        "    ]\n",
        "else:\n",
        "    launchArguments += [\n",
        "    '--mixed_precision',\n",
        "    'no',\n",
        "  ]\n",
        "if vramSaving == 'lossy' or pruneOutput:\n",
        "  if hasBf16:\n",
        "    launchArguments += [\n",
        "      '--save_precision',\n",
        "      'bf16',\n",
        "    ]\n",
        "  else:\n",
        "    launchArguments += [\n",
        "      '--save_precision',\n",
        "      'fp16',\n",
        "    ]\n",
        "else:\n",
        "  launchArguments += [\n",
        "    '--save_precision',\n",
        "    'float',\n",
        "  ]\n",
        "if samplePrompt:\n",
        "  launchArguments += [\n",
        "    '--sample_sampler',\n",
        "    'euler_a',\n",
        "    '--sample_prompts',\n",
        "    'prompt.txt',\n",
        "    '--sample_every_n_epochs',\n",
        "    1,\n",
        "  ]\n",
        "if wandbKey:\n",
        "  launchArguments += [\n",
        "    '--log_with',\n",
        "    'wandb',\n",
        "    '--wandb_api_key',\n",
        "    wandbKey,\n",
        "  ]\n",
        "launchArguments += [\n",
        "  '--metadata_title',\n",
        "  'SinansWoche',\n",
        "]\n",
        "\n",
        "import shlex\n",
        "segments = [\n",
        "  'accelerate',\n",
        "  'launch',\n",
        "] + accelerateArguments + [\n",
        "  os.path.join(repoFolder, 'sdxl_train_network.py'),\n",
        "] + launchArguments\n",
        "command = \" \".join(shlex.quote(str(segment)) for segment in segments)\n",
        "print(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Clean memory\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "# @title Collect states\n",
        "%mkdir --parents samples\n",
        "stepModels = []\n",
        "for filename in os.listdir(modelFolder):\n",
        "  nameWithoutSuffix = os.path.splitext(filename)[0]\n",
        "  filepath = os.path.join(modelFolder, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    if re.match(r'.+-[0-9]{6}$', nameWithoutSuffix):\n",
        "      stepModels.append(filename)\n",
        "stepModels.sort()\n",
        "stepModels.append('trained.safetensors')\n",
        "print(stepModels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "# DPMSolverMultistepScheduler.from_config(\n",
        "#             scheduler_config, use_karras_sigmas=True, algorithm_type=\"sde-dpmsolver++\"\n",
        "#         )\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    'stabilityai/stable-diffusion-xl-base-1.0',\n",
        ")\n",
        "pipe.to('cuda')\n",
        "pipe.enable_freeu(0.6, 0.4, 1.1, 1.2)\n",
        "prompt = 'SinansWoche sitting on a chair' # @param {type:\"string\"}\n",
        "image = pipe(\n",
        "prompt=prompt,\n",
        "guidance_scale=6,\n",
        "num_inference_steps=100,\n",
        "output_type='pil',\n",
        ").images[0]\n",
        "image.save('samples/sample.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Display image\n",
        "from IPython.display import Image\n",
        "Image('samples/sample.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
